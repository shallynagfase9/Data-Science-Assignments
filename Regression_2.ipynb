{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2hbCSHdfv5k6hB9Mzie2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Data-Science-Assignments/blob/main/Regression_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
      ],
      "metadata": {
        "id": "rDAZEjH46X1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "R-squared, also known as the coefficient of determination, is a statistical measure used in linear regression models to evaluate the goodness of fit.\n",
        "\n",
        "\n",
        "R-squared is calculated using the following formula:\n",
        "R2 = 1 - SSres / SStot\n",
        "​\n",
        "\"\"\"\n",
        "\n",
        "​\n"
      ],
      "metadata": {
        "id": "PeChcUKg6VEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n"
      ],
      "metadata": {
        "id": "N2wMekTE6zMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model.\n",
        "While R-squared increases whenever a new predictor is added to the model, adjusted R-squared takes into account the number of predictors and only increases if the new predictor improves the model more than would be expected by chance.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EZgukv-L61Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. When is it more appropriate to use adjusted R-squared?"
      ],
      "metadata": {
        "id": "P96yqkAI6-2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Adjusted R-squared is more appropriate than regular R-squared when you have multiple predictors, are comparing models, are involved in model selection processes, want to prevent overfitting,\n",
        "or are working with complex data sets. It provides a more nuanced and accurate evaluation of model performance by considering the number of predictors used.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "X6OFu3717Afx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
        "calculated, and what do they represent?"
      ],
      "metadata": {
        "id": "zdYKZuWc7Wh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MSE: Measures the average squared error between actual and predicted values. Sensitive to outliers.\n",
        "RMSE: Square root of MSE, providing error magnitude in the same units as the data. Also sensitive to outliers.\n",
        "MAE: Measures the average absolute error between actual and predicted values. Less sensitive to outliers.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nvNwW3dI7YNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
        "regression analysis."
      ],
      "metadata": {
        "id": "jipvrH1y7ppf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MSE:\n",
        "Advantages: Penalizes large errors, useful for optimization.\n",
        "Disadvantages: Highly sensitive to outliers, less interpretable due to squared units.\n",
        "\n",
        "RMSE:\n",
        "Advantages: Provides error magnitude in the same units as the data, penalizes large errors.\n",
        "Disadvantages: Sensitive to outliers, more complex calculation.\n",
        "\n",
        "MAE:\n",
        "Advantages: Simple to understand and calculate, robust to outliers.\n",
        "Disadvantages: Linear penalty for errors, not differentiable at zero.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yB5Wn_to7rvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
        "it more appropriate to use?"
      ],
      "metadata": {
        "id": "G7vkJ1u-8A1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to enhance the model by adding a penalty for large coefficients.\n",
        "This helps in reducing overfitting and can also perform variable selection by shrinking some coefficients to exactly zero, effectively removing them from the model.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AFhT-7e88Bb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
        "example to illustrate."
      ],
      "metadata": {
        "id": "1xXE2pef8R6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Regularized linear models help to prevent overfitting by adding a penalty to the loss function for large coefficients, thereby discouraging complex models that fit the training data too closely.\n",
        "\n",
        "Example to Illustrate Regularization-\n",
        "Suppose we are building a linear regression model to predict house prices based on various features such as square footage, number of bedrooms, age of the house, and more.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zYBYskhQ8UDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
        "choice for regression analysis."
      ],
      "metadata": {
        "id": "tINA6UOa8mDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Regularized linear models are powerful tools for handling overfitting and improving generalization. However, their limitations in terms of model interpretability, handling non-linear relationships, and computational cost should be carefully considered.\n",
        "In some scenarios, other regression methods may be more appropriate, depending on the specific characteristics and requirements of the task at hand.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "FEwPd3_K8oRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
        "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
        "performer, and why? Are there any limitations to your choice of metric?"
      ],
      "metadata": {
        "id": "A21oRhKa85Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If the dataset is likely to contain outliers and a robust measure of typical error is preferred, Model B with an MAE of 8 might be better.\n",
        "If large errors need to be minimized and outliers are not a significant concern, Model A with an RMSE of 10 might be preferable.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vi39Hdux88EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. You are comparing the performance of two regularized linear models using different types of\n",
        "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
        "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
        "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
        "method?"
      ],
      "metadata": {
        "id": "M8wuT6Cl9KqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Without specific performance metrics (such as MSE, RMSE, or MAE) for both models on a validation set, it's challenging to definitively determine which model is better. However, based on the provided regularization parameters:\n",
        "\n",
        "If interpretability and feature selection are crucial, Model B (Lasso) with a higher λ of 0.5 might be preferred.\n",
        "\n",
        "If preserving all predictors and reducing variance while controlling bias are priorities, Model A (Ridge) with a lower λ of 0.1 could be chosen.\n",
        "\n",
        "Ultimately, the decision should be guided by the specific requirements of the problem, the trade-offs between bias and variance, and the interpretability of the resulting model.\n",
        "Regularization methods provide powerful tools to manage model complexity and improve generalization, but their effectiveness depends on careful parameter tuning and understanding of the data characteristics.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "HCo678Ok9MNx",
        "outputId": "4dfeba64-2606-4899-fd19-f0792540030e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWithout specific performance metrics (such as MSE, RMSE, or MAE) for both models on a validation set, it's challenging to definitively determine which model is better. However, based on the provided regularization parameters:\\n\\nIf interpretability and feature selection are crucial, Model B (Lasso) with a higher λ of 0.5 might be preferred.\\n\\nIf preserving all predictors and reducing variance while controlling bias are priorities, Model A (Ridge) with a lower λ of 0.1 could be chosen.\\n\\nUltimately, the decision should be guided by the specific requirements of the problem, the trade-offs between bias and variance, and the interpretability of the resulting model. \\nRegularization methods provide powerful tools to manage model complexity and improve generalization, but their effectiveness depends on careful parameter tuning and understanding of the data characteristics.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLmnMOEn9WLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}