{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyq4Ct2sEXqlZ48Aco6AG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Data-Science-Assignments/blob/main/Regression_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZWi2HvXziflQ",
        "outputId": "9b79eae8-3f2e-4a43-f481-f62363b0b52e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nSimple linear regression models the relationship between a single dependent variable and a independent variable using a straight line (Best fit line).\\nExample: Predicting a person's weight based on their height.\\n\\nMultiple linear regression models the relationship between two or more dependent variables and a independent variable.\\nExample: Predicting a person's weight based on their height and age.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
        "\"\"\"\n",
        "Simple linear regression models the relationship between a single dependent variable and a independent variable using a straight line (Best fit line).\n",
        "Example: Predicting a person's weight based on their height.\n",
        "\n",
        "Multiple linear regression models the relationship between two or more dependent variables and a independent variable.\n",
        "Example: Predicting a person's weight based on their height and age.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "\"\"\"\n",
        "Assumptions of Linear Regression\n",
        "\n",
        "- Linearity: The relationship between the predictors and the response variable is linear.\n",
        "- Independence: The residuals (errors) are independent of each other.\n",
        "- Homoscedasticity: The residuals have constant variance at every level of the predictor variables.\n",
        "- Normality: The residuals of the model are normally distributed.\n",
        "- No Multicollinearity: The predictor variables are not highly correlated with each other.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "1W21RyhcjUlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
        "\n",
        "\"\"\"\n",
        "In a linear regression model, the slope and intercept are key parameters that define the relationship between the predictor variable(s) and the response variable. Here‚Äôs how you interpret them:\n",
        "\n",
        "Intercept(ùõΩ0) :\n",
        "Definition: The intercept is the expected value of the response variable when all predictor variables are zero.\n",
        "Interpretation: It represents the baseline level of the response variable in the absence of any effect from the predictors.\n",
        "\n",
        "Slope (ùõΩ1,ùõΩ2):\n",
        "Definition: The slope is the change in the response variable for a one-unit change in the predictor variable, holding all other predictors constant.\n",
        "Interpretation: It represents the rate of change of the response variable with respect to the predictor variable.\n",
        "\n",
        "Real-World Example - Predicting house prices based on the size of the house.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "9MdFY22cjUn2",
        "outputId": "4f9828f5-54a8-41aa-ee42-1e478984bd41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIn a linear regression model, the slope and intercept are key parameters that define the relationship between the predictor variable(s) and the response variable. Here‚Äôs how you interpret them:\\n\\nIntercept(ùõΩ0) :\\nDefinition: The intercept is the expected value of the response variable when all predictor variables are zero.\\nInterpretation: It represents the baseline level of the response variable in the absence of any effect from the predictors.\\n\\nSlope (ùõΩ1,ùõΩ2):\\nDefinition: The slope is the change in the response variable for a one-unit change in the predictor variable, holding all other predictors constant.\\nInterpretation: It represents the rate of change of the response variable with respect to the predictor variable.\\n\\nReal-World Example - Predicting house prices based on the size of the house.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "akbm_mPOkqcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "\"\"\"\n",
        "Gradient descent is an optimization algorithm used to minimize the cost function (or loss function) in machine learning models.\n",
        "It is a fundamental technique for training many machine learning algorithms, including linear regression, logistic regression, and neural networks.\n",
        "\n",
        "Concept of Gradient Descent-\n",
        "The basic idea of gradient descent is to iteratively adjust the parameters of the model to find the parameter values that minimize the cost function. Here's how it works:\n",
        "\n",
        "- Initialize Parameters: Start with initial guesses for the model parameters. These can be random values or zeros.\n",
        "- Compute the Cost: Calculate the cost function, which measures how well the model is performing with the current parameters. In the case of linear regression, the cost function is typically the mean squared error (MSE).\n",
        "- Compute the Gradients: Calculate the gradients of the cost function with respect to each parameter. The gradient is a vector of partial derivatives, representing the slope of the cost function in each parameter's direction.\n",
        "- Update the Parameters: Adjust the parameters in the direction opposite to the gradient. This step is taken to reduce the cost function. The size of the adjustment is controlled by the learning rate (ùõº).\n",
        "- Iterate: Repeat steps 2-4 until the cost function converges to a minimum value, meaning the changes in the cost function between iterations become very small.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "-WNLsam6jUqQ",
        "outputId": "c4f83229-2867-4797-c877-8eb88870ebb7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nGradient descent is an optimization algorithm used to minimize the cost function (or loss function) in machine learning models. \\nIt is a fundamental technique for training many machine learning algorithms, including linear regression, logistic regression, and neural networks.\\n\\nConcept of Gradient Descent-\\nThe basic idea of gradient descent is to iteratively adjust the parameters of the model to find the parameter values that minimize the cost function. Here's how it works:\\n\\n- Initialize Parameters: Start with initial guesses for the model parameters. These can be random values or zeros.\\n- Compute the Cost: Calculate the cost function, which measures how well the model is performing with the current parameters. In the case of linear regression, the cost function is typically the mean squared error (MSE).\\n- Compute the Gradients: Calculate the gradients of the cost function with respect to each parameter. The gradient is a vector of partial derivatives, representing the slope of the cost function in each parameter's direction.\\n- Update the Parameters: Adjust the parameters in the direction opposite to the gradient. This step is taken to reduce the cost function. The size of the adjustment is controlled by the learning rate (ùõº).\\n- Iterate: Repeat steps 2-4 until the cost function converges to a minimum value, meaning the changes in the cost function between iterations become very small.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "\"\"\"\n",
        "Multiple linear regression is a statistical technique that models the relationship between a response variable and multiple predictor variables. It generalizes the simple linear regression model by allowing for more than one predictor.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9fI-5pgDjUs8",
        "outputId": "b98ceb94-b882-4a75-d65c-1c1d66d5a009"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMultiple linear regression is a statistical technique that models the relationship between a response variable and multiple predictor variables. It generalizes the simple linear regression model by allowing for more than one predictor.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "\"\"\"\n",
        "Multicollinearity occurs in multiple linear regression when two or more predictor variables are highly correlated with each other.\n",
        "This means that one predictor variable can be linearly predicted from the others with a high degree of accuracy.\n",
        "Multicollinearity can cause problems in the regression analysis because it makes it difficult to isolate the individual effect of each predictor on the response variable.\n",
        "As a result, the coefficients of the regression model can become unstable and have large standard errors.\n",
        "\n",
        "Addressing Multicollinearity\n",
        "- Remove Highly Correlated Predictors: If two predictors are highly correlated, consider removing one of them from the model.\n",
        "- Combine Predictors: Combine correlated predictors into a single predictor through techniques such as principal component analysis (PCA).\n",
        "- Regularization Techniques: Use regularization methods like Ridge Regression or Lasso Regression, which add a penalty to the regression to shrink the coefficients of correlated predictors.\n",
        "- Ridge Regression: Adds a penalty equivalent to the square of the magnitude of coefficients.\n",
        "- Lasso Regression: Adds a penalty equivalent to the absolute value of the magnitude of coefficients, which can lead to sparse models where some coefficients are exactly zero.\n",
        "- Centering the Predictors: Centering (subtracting the mean) can help reduce multicollinearity by making the predictors more orthogonal.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "P90m9V1HjUvh",
        "outputId": "a28a3cda-c15f-4e46-e686-97db8435c48d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMulticollinearity occurs in multiple linear regression when two or more predictor variables are highly correlated with each other. \\nThis means that one predictor variable can be linearly predicted from the others with a high degree of accuracy. \\nMulticollinearity can cause problems in the regression analysis because it makes it difficult to isolate the individual effect of each predictor on the response variable. \\nAs a result, the coefficients of the regression model can become unstable and have large standard errors.\\n\\nAddressing Multicollinearity\\n- Remove Highly Correlated Predictors: If two predictors are highly correlated, consider removing one of them from the model.\\n- Combine Predictors: Combine correlated predictors into a single predictor through techniques such as principal component analysis (PCA).\\n- Regularization Techniques: Use regularization methods like Ridge Regression or Lasso Regression, which add a penalty to the regression to shrink the coefficients of correlated predictors.\\n- Ridge Regression: Adds a penalty equivalent to the square of the magnitude of coefficients.\\n- Lasso Regression: Adds a penalty equivalent to the absolute value of the magnitude of coefficients, which can lead to sparse models where some coefficients are exactly zero.\\n- Centering the Predictors: Centering (subtracting the mean) can help reduce multicollinearity by making the predictors more orthogonal.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "\"\"\"\n",
        "Polynomial regression is a type of regression analysis in which the relationship between the independent variable ùëã and the dependent variable ùëå is modeled as an n-th degree polynomial.\n",
        "It extends the linear regression model to capture non-linear relationships by adding polynomial terms of the predictor variable(s).\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Wz_-wX7ZmL84",
        "outputId": "fb24cc92-a341-4a69-e1b2-1c24599353ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPolynomial regression is a type of regression analysis in which the relationship between the independent variable ùëã and the dependent variable ùëå is modeled as an n-th degree polynomial. \\nIt extends the linear regression model to capture non-linear relationships by adding polynomial terms of the predictor variable(s).\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
        "\"\"\"\n",
        "Advantages of Polynomial Regression\n",
        "- Flexibility: By including higher-degree polynomial terms, the model becomes more flexible and can fit a wide range of data patterns.\n",
        "- Improved Fit: For certain datasets, polynomial regression can provide a better fit than linear regression by reducing the residual sum of squares (RSS) and improving the model's performance.\n",
        "\n",
        "Disadvantages of Polynomial Regression\n",
        "- Overfitting: Higher-degree polynomials can lead to overfitting, where the model fits the noise in the data rather than the underlying trend. This can result in poor generalization to new data.\n",
        "- Complexity:  As the degree of the polynomial increases, the model becomes more complex, making it harder to interpret and understand.\n",
        "- Extrapolation Issues: Polynomial models can exhibit extreme behavior outside the range of the data, making them unreliable for extrapolation.\n",
        "- Multicollinearity: Polynomial terms can be highly correlated with each other, leading to multicollinearity, which can inflate the variance of the coefficient estimates.\n",
        "\n",
        "Situations to Prefer Polynomial Regression\n",
        "- Non-Linear Relationships: When the relationship between the predictor and response variable is non-linear and cannot be adequately captured by a linear model.\n",
        "- Data Patterns: When exploratory data analysis (e.g., scatter plots) indicates a polynomial pattern in the data.\n",
        "- Model Fit: When a linear model fails to provide a satisfactory fit, and higher-degree polynomial terms improve model performance metrics (e.g., lower RSS, higher R-squared).\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "1MH_o_UImjom",
        "outputId": "24c833c2-7242-420b-8d8c-be4a81b5c265"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nAdvantages of Polynomial Regression\\n- Flexibility: By including higher-degree polynomial terms, the model becomes more flexible and can fit a wide range of data patterns.\\n- Improved Fit: For certain datasets, polynomial regression can provide a better fit than linear regression by reducing the residual sum of squares (RSS) and improving the model's performance.\\n\\nDisadvantages of Polynomial Regression\\n- Overfitting: Higher-degree polynomials can lead to overfitting, where the model fits the noise in the data rather than the underlying trend. This can result in poor generalization to new data.\\n- Complexity:  As the degree of the polynomial increases, the model becomes more complex, making it harder to interpret and understand.\\n- Extrapolation Issues: Polynomial models can exhibit extreme behavior outside the range of the data, making them unreliable for extrapolation.\\n- Multicollinearity: Polynomial terms can be highly correlated with each other, leading to multicollinearity, which can inflate the variance of the coefficient estimates.\\n\\nSituations to Prefer Polynomial Regression\\n- Non-Linear Relationships: When the relationship between the predictor and response variable is non-linear and cannot be adequately captured by a linear model.\\n- Data Patterns: When exploratory data analysis (e.g., scatter plots) indicates a polynomial pattern in the data.\\n- Model Fit: When a linear model fails to provide a satisfactory fit, and higher-degree polynomial terms improve model performance metrics (e.g., lower RSS, higher R-squared).\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpvVOaAXnQuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KsBmR0G5jLX-"
      }
    }
  ]
}