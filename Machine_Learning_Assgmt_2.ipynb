{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNih0MPtEC81JM9l81xlWAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Data-Science-Assignments/blob/main/Machine_Learning_Assgmt_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgRAk73cOT6J"
      },
      "outputs": [],
      "source": [
        "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
        "\"\"\"\n",
        "Overfitting and underfitting are two common issues in machine learning that can negatively impact the performance of a model.\n",
        "\n",
        "Overfitting occurs when a model performs well on the training data but poorly on new, unseen data (test data).\n",
        "Overfitting can be caused by low bias and high variance, a small training dataset, or a model that is too complex for the data.\n",
        "To mitigate overfitting, techniques such as regularization, cross-validation, and ensembling can be used.\n",
        "\n",
        "Underfitting, results in a model that performs poorly on both the training and new, unseen data (test data).\n",
        "Underfitting can be caused by high bias, a small training dataset, or a model that is too simple for the data.\n",
        "To mitigate underfitting, techniques such as increasing the complexity of the model, increasing the size of the training dataset, and reducing noise in the data can be used.\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: How can we reduce overfitting? Explain in brief.\n",
        "\"\"\"\n",
        "Overfitting can be caused by low bias and high variance, a small training dataset, or a model that is too complex for the data.\n",
        "To mitigate overfitting, techniques such as regularization, cross-validation, and ensembling can be used.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7sRGD-PKPWZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "\"\"\"\n",
        "Underfitting in machine learning refers to a model that is too simple to capture the underlying relationship in the dataset it is trained on.\n",
        "This results in poor performance on both the training set and unseen data.\n",
        "Underfitting is more common in certain business contexts where labeled training data may be sparse.\n",
        "\n",
        "Some scenarios where underfitting can occur in machine learning include:\n",
        "1) Complex models not trained for long enough: If complex models such as neural networks are not trained for long enough, they may underfit the data.\n",
        "2) Insufficient number of training samples: If a model is not provided with a sufficient number of training samples, it may underfit due to the uncertainty in the training data.\n",
        "3) High bias: By far the most common reason that models underfit is because they exhibit too much bias.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q1AvHM8hPiap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
        "\"\"\"\n",
        "The bias-variance tradeoff is a fundamental concept in machine learning that deals with the balance between model bias and variance. In simpler terms, it refers to the tradeoff between a model's ability to accurately represent the underlying data patterns (low bias) and its susceptibility to fluctuations with changes in the training data (high variance).\n",
        "\n",
        "The relationship between bias and variance is that as the complexity of a model increases, its variance decreases, and its bias increases. Conversely, as the complexity of a model decreases, its variance increases, and its bias decreases. The goal is to find the right balance between these two aspects to create a model that performs well on new, unseen data.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_-_S2HlFP3WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
        "\"\"\"\n",
        "To detect overfitting, one common method is to compare the training accuracy and cross-validation accuracy. If the training accuracy is significantly higher than the cross-validation accuracy, it is likely that the model is overfitting. Another method is to use a learning curve, which plots the training and validation loss of a sample of training examples by incrementally adding new training data. If the model is overfitting, adding additional training examples may improve the model performance on unseen data.\n",
        "To detect underfitting, one can look at the training and validation loss on a learning curve. If the training and validation loss are close to each other at the end, it may indicate that the model is underfitting. Additionally, if the training loss is high and the validation loss is also high, it may indicate that the model is underfitting.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zXLa6e5rQKi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
        "\"\"\"\n",
        "Bias and variance are two important concepts in machine learning that refer to the errors that can occur during the training and testing phases of a model.\n",
        "\n",
        "Examples of high bias models include linear regression models that assume a linear relationship between variables, or decision trees that are too shallow and fail to capture complex patterns in the data.\n",
        "\n",
        "Examples of high variance models include decision trees that are too deep and overfit to the training data, or neural networks with too many layers or parameters.\n",
        "\n",
        "To mitigate high bias, one can use more complex models, incorporate more features, or use regularization techniques to prevent overfitting.\n",
        "To mitigate high variance, one can use simpler models, reduce the number of features, or use regularization techniques to prevent overfitting.\n",
        "\n",
        "Ensemble methods, such as bagging and boosting, can also be used to reduce variance and improve generalization performance.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hLZ-SSXzQc5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
        "\"\"\"\n",
        "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function, discouraging the model from assigning too much importance to individual features or coefficients.\n",
        "This helps to reduce the model's complexity and improve its generalization ability.\n",
        "\n",
        "There are two main types of regularization techniques: L1 and L2 regularization.\n",
        "L1 regularization, also known as Lasso regularization, adds a penalty term that is equal to the absolute value of the coefficients.\n",
        "This can lead to some coefficients being reduced to zero, making it useful for compressing the model.\n",
        "L2 regularization, also known as Ridge regularization, adds a penalty term that is equal to the square of the coefficients.\n",
        "This encourages the coefficients to be small, but not necessarily zero.\n",
        "\n",
        "Regularization techniques are used to make models more generalizable and reduce the likelihood of training models that are overly complex or have too many parameters relative to the amount of data they are trained on.\n",
        "By introducing a penalty term to the loss function, regularization helps to reduce the variance of the model and prevent overfitting.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "T28hRIyzQ7-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}